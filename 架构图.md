# ExRpc 架构图文档

本文档包含 ExRpc 框架的完整架构图，帮助理解各个类和组件在不同场景下的调用关系。

---

## 目录

1. [序列图 - 客户端RPC调用流程](#1-序列图---客户端rpc调用流程)
2. [序列图 - 集群节点变更流程](#2-序列图---集群节点变更流程)
3. [类图 - 核心通信架构](#3-类图---核心通信架构)
4. [类图 - 服务器端架构](#4-类图---服务器端架构)
5. [类图 - 消息事件架构](#5-类图---消息事件架构)
6. [组件图 - 模块依赖关系](#6-组件图---模块依赖关系)
7. [流程图 - 集群模式选择](#7-流程图---集群模式选择)
8. [流程图 - 重试逻辑](#8-流程图---重试逻辑)
9. [状态图 - Transaction状态机](#9-状态图---transaction状态机)
10. [架构总览图](#10-架构总览图)

---

## 1. 序列图 - 客户端RPC调用流程

展示从客户端发起远程调用到收到服务端响应的完整时序。

```mermaid
sequenceDiagram
    participant Client as 客户端应用
    participant Proxy as ObjectProxyXxx<br/>(生成的代理)
    participant ProxyBase as ObjectProxyBase
    participant Comm as Communicator
    participant Trans as Transaction
    participant Pool as TcpClientPool
    participant Server as RPCServerHost
    participant RpcServer as RPCServerBase
    participant Servant as ServantBase

    Client->>Proxy: RemoteMethod(params)
    Proxy->>ProxyBase: BeginRpcTransaction(request)
    ProxyBase->>Trans: new Transaction()
    ProxyBase->>ProxyBase: CommonSendWithRetry()

    loop 重试逻辑 (最多3次)
        ProxyBase->>Comm: SendMessage(request, trans)
        Comm->>Pool: GetClient(host, port)
        Pool-->>Comm: TcpClient
        Comm->>Pool: SendBytesAsync(data)
        Pool->>Trans: 触发 send_evt
        Trans-->>ProxyBase: 发送完成

        alt 发送成功
            ProxyBase->>Trans: 等待 ack_evt (超时9秒)
            Pool->>Server: TCP传输请求
            Server->>Server: OnClientReceivedMessage()
            Server->>RpcServer: CallServantMethod(request)
            RpcServer->>Servant: call_method(request, out response)
            Servant->>Servant: 执行业务逻辑
            Servant-->>RpcServer: response
            RpcServer->>RpcServer: 记录性能指标
            RpcServer-->>Server: response
            Server->>Pool: SendMessageToClient(response)
            Pool->>Comm: OnReceivedServerData(response)
            Comm->>Trans: 触发 ack_evt
            Trans-->>ProxyBase: 响应到达
            ProxyBase-->>Proxy: response
            Proxy-->>Client: 返回结果
        else 发送失败
            ProxyBase->>ProxyBase: 指数退避延迟
        end
    end

    ProxyBase->>Comm: UnRegisterTransaction()
```

**关键点：**
- Transaction 管理整个请求的生命周期
- 支持重试机制（默认2次重试）
- 使用事件机制（send_evt、ack_evt）进行同步
- 超时时间可配置（默认9秒）

---

## 2. 序列图 - 集群节点变更流程

展示当 ZooKeeper 检测到集群拓扑变化时，系统如何响应和更新。

```mermaid
sequenceDiagram
    participant ZK as ZooKeeper
    participant CC as ClusterClient
    participant CI as ClusterInvokerBase<br/>(客户端)
    participant RS as RPCServerBase<br/>(服务端)
    participant Comm as Communicator

    ZK->>ZK: 节点加入/离开
    ZK->>CC: ChildrenChangedWatcher 触发
    CC->>CC: OnClusterNodesChangedEventHandler()
    CC->>ZK: GetClusterNodes()
    ZK-->>CC: ClusterNodeInfo[] (JSON)
    CC->>CC: 反序列化节点信息

    alt 集群模式 = ClusterWithHash
        CC->>CC: RebuildNodeInstanceIdHashTable()
        Note over CC: 一致性哈希重新分配<br/>最小化key移动
    end

    CC->>CI: _OnClusterNodeChangedHanlder(nodes)
    CI->>CI: 更新 _ClusterNodes
    CI->>Comm: 更新缓存的 Communicator[]

    CC->>RS: _OnClusterNodeChangedHanlder(nodes)
    RS->>RS: 更新 _ClusterNodes
    RS->>RS: 更新哈希表映射

    Note over CI,RS: 客户端和服务端都获得<br/>最新的集群拓扑
```

**关键点：**
- ZooKeeper 作为集群配置中心
- ClusterClient 负责监听和处理拓扑变化
- 一致性哈希在节点变化时自动重新平衡
- 客户端和服务端同步更新集群信息

---

## 3. 类图 - 核心通信架构

展示客户端通信层的核心类及其关系。

```mermaid
classDiagram
    class IObjectProxy {
        <<interface>>
        +GetObjectName() string
        +SetHost(host, port)
    }

    class IRpcCallMethod {
        <<interface>>
        +__cb_id: long
        +__tid: string
        +__servant: string
        +__method: string
    }

    class IRpcCallMethodReturn {
        <<interface>>
        +__err: int
        +__cb_id: long
        +__tid: string
    }

    class CommunicatorFactory {
        <<singleton>>
        -_Communicators: ConcurrentDictionary
        +GetCommunicator(name) Communicator
        +LoadFromXml(configPath)
    }

    class Communicator {
        -_TransactionTable: ConcurrentDictionary~long, Transaction~
        -_TcpClientPools: Dictionary~string, TcpClientPool~
        -_Config: CommunicatorConfigure
        +SendMessage(request, trans)
        +RegisterTransaction(trans)
        +UnRegisterTransaction(trans)
        -CleanupExpiredTransactions()
        -OnReceivedServerData(response)
    }

    class ObjectProxyBase {
        #_Comm: Communicator
        #_Host: string
        #_Port: int
        +BeginRpcTransaction(request, trans) response
        #CommonSendWithRetry() bool
    }

    class Transaction {
        +CallbackID: long
        +ThreadID: int
        +send_evt: AutoResetEvent
        +ack_evt: AutoResetEvent
        +status: TransactionStatus
        +Request: IRpcCallMethod
        +Response: IRpcCallMethodReturn
        +Expire: DateTime
    }

    class TcpClientPool {
        -_Clients: TcpClient[]
        -_PoolSize: int
        +GetClient() TcpClient
        +SendBytesAsync(data)
    }

    class GridUri {
        +Cluster: string
        +Project: string
        +Root: string
        +Parse(uri) GridUri
    }

    CommunicatorFactory --> Communicator : 创建和管理
    ObjectProxyBase --> Communicator : 使用
    ObjectProxyBase ..|> IObjectProxy : 实现
    ObjectProxyBase --> Transaction : 创建
    ObjectProxyBase --> IRpcCallMethod : 发送
    ObjectProxyBase --> IRpcCallMethodReturn : 接收
    Communicator --> Transaction : 注册/注销
    Communicator --> TcpClientPool : 管理连接池
    Transaction --> IRpcCallMethod : 包含
    Transaction --> IRpcCallMethodReturn : 包含
```

**关键关系：**
- `CommunicatorFactory` 单例模式管理 `Communicator` 实例
- `ObjectProxyBase` 依赖 `Communicator` 发送请求
- `Transaction` 封装请求/响应和同步事件
- `TcpClientPool` 管理TCP连接复用

---

## 4. 类图 - 服务器端架构

展示服务端处理请求的核心类及其关系。

```mermaid
classDiagram
    class IServer {
        <<interface>>
        +Start()
        +Stop()
        +CallServantMethod(request) response
        +RegisterServant(servant)
        +SetLogger(logger)
    }

    class IServant {
        <<interface>>
        +call_method(request, out response) bool
        +GetServantName() string
    }

    class TcpServiceBase {
        <<abstract>>
        #OnClientReceivedMessage(socket, message)
        #SendMessageToClient(socket, message)
        +Start()
        +Stop()
    }

    class RPCServerHost {
        -_Servers: List~IServer~
        -_HostInfos: List~HostInfo~
        -_Config: ServerHostConfigure
        +LoadServantAssembly(path)
        +LoadRpcAssembly(path)
        +InitServers()
        #OnClientReceivedMessage(socket, message)
    }

    class RPCServerBase {
        -_ServantTable: Dictionary~string, IServant~
        -_ClusterNodes: ClusterNodeInfo[]
        -_ClusterClient: ClusterClient
        -_PerformanceRecorder: PerformanceRecorder
        +RegisterServant(servant)
        +CallServantMethod(request) response
        +StartClusterNode(mode)
        -InitServantTable()
    }

    class ServantBase {
        <<abstract>>
        #_Logger: ILogger
        +call_method(request, out response) bool
        #Error(msg)
        #Info(msg)
    }

    class ClusterClient {
        -_ClusterNodes: ClusterNodeInfo[]
        -_NodeInstanceIdHashTable: Dictionary
        -_ZKClient: ZKClient
        +GetClusterNodes() ClusterNodeInfo[]
        +GetMasterClusterNode() ClusterNodeInfo
        +GetSlaveClusterNode() ClusterNodeInfo
        +GetClusterNodeByHash(value) ClusterNodeInfo
        +GetClusterNodeByMod(value) ClusterNodeInfo
        -OnClusterNodesChangedEventHandler()
        -RebuildNodeInstanceIdHashTable()
    }

    class ClusterNodeInfo {
        +Host: string
        +Port: int
        +IsMaster: bool
        +InstanceId: string
        +ClusterMode: ClusterMode
    }

    TcpServiceBase <|-- RPCServerHost : 继承
    IServer <|.. RPCServerBase : 实现
    IServant <|.. ServantBase : 实现
    RPCServerHost --> RPCServerBase : 管理多个
    RPCServerBase --> ServantBase : 管理多个
    RPCServerBase --> ClusterClient : 使用
    ClusterClient --> ClusterNodeInfo : 管理
```

**关键关系：**
- `RPCServerHost` 继承 `TcpServiceBase` 处理网络层
- `RPCServerBase` 实现 `IServer` 接口，管理 Servant
- `ServantBase` 实现 `IServant` 接口，执行业务逻辑
- `ClusterClient` 管理集群拓扑和节点选择策略

---

## 5. 类图 - 消息事件架构

展示基于 RabbitMQ 的发布/订阅消息系统。

```mermaid
classDiagram
    class IEvent {
        <<interface>>
        +Sender: object
    }

    class ExEventManager {
        <<static>>
        -_EventsTable: ConcurrentDictionary~string, ExEventMeta~
        +GetEventMeta(eventName) ExEventMeta
        +LoadFromXml(configPath)
    }

    class ExEventMeta {
        +EventName: string
        +Exchange: string
        +QueuePrefix: string
        +RabbitMQUri: string
        +IsPersistence: bool
        +Credentials: Credential[]
    }

    class EventClient {
        <<abstract>>
        #_EventMeta: ExEventMeta
        #_Credential: Credential
        +EventClient(eventName, credential)
        #ValidatePermission()
    }

    class Publisher {
        -_TotalCount: long
        -_TotalSucc: long
        +LaunchEvent(event, routingKey)
        +LaunchEvents(events, routingKey)
        -GetRabbitMQClient() RabbitMQClient
    }

    class Subscriber {
        -_ReceivedCount: long
        +ListenEvent(handler, args, routingKey)
        +Dispose()
    }

    class RabbitMQClientFactory {
        <<static>>
        -_ClientPool: ConcurrentDictionary
        +GetClient(uri, exchange) RabbitMQClient
        +ReturnClient(client)
    }

    class RabbitMQClient {
        -_Connection: IConnection
        -_Channel: IModel
        -_Exchange: string
        +BasicPublish(routingKey, message, mode)
        +StartConsumer(queue, handler, args)
        +Dispose()
    }

    ExEventManager --> ExEventMeta : 管理
    EventClient --> ExEventMeta : 使用
    EventClient <|-- Publisher : 继承
    EventClient <|-- Subscriber : 继承
    Publisher --> RabbitMQClientFactory : 获取客户端
    Subscriber --> RabbitMQClientFactory : 获取客户端
    RabbitMQClientFactory --> RabbitMQClient : 创建和管理
    Publisher --> IEvent : 发布
    Subscriber --> IEvent : 订阅
```

**关键关系：**
- `ExEventManager` 单例管理事件元数据
- `EventClient` 作为发布者和订阅者的基类
- `RabbitMQClientFactory` 管理客户端连接池
- `Publisher` 和 `Subscriber` 通过 `RabbitMQClient` 与 RabbitMQ 交互

---

## 6. 组件图 - 模块依赖关系

展示 ExRpc 各个模块之间的依赖关系和职责划分。

```mermaid
graph TB
    subgraph "ExRpc.Common - 核心RPC框架"
        subgraph "客户端层"
            CommunicatorFactory[CommunicatorFactory<br/>通信工厂]
            Communicator[Communicator<br/>通信管理器]
            ObjectProxyBase[ObjectProxyBase<br/>代理基类]
            ClusterInvokerBase[ClusterInvokerBase<br/>集群调用器]
        end

        subgraph "服务端层"
            RPCServerHost[RPCServerHost<br/>服务器主机]
            RPCServerBase[RPCServerBase<br/>RPC服务器]
            ServantBase[ServantBase<br/>业务处理器]
        end

        subgraph "集群层"
            ClusterClient[ClusterClient<br/>集群客户端]
            GridUri[GridUri<br/>服务地址]
            ClusterNodeInfo[ClusterNodeInfo<br/>节点信息]
        end

        subgraph "通信层"
            Transaction[Transaction<br/>事务]
            TcpClientPool[TcpClientPool<br/>连接池]
            JMPParser[JMPParser<br/>消息解析器]
        end
    end

    subgraph "ExRpc.Messaging - 消息系统"
        ExEventManager[ExEventManager<br/>事件管理器]
        Publisher[Publisher<br/>发布者]
        Subscriber[Subscriber<br/>订阅者]
        RabbitMQClient[RabbitMQClient<br/>RabbitMQ客户端]
    end

    subgraph "外部依赖"
        ZooKeeper[ZooKeeperNetEx<br/>服务发现]
        RabbitMQ[RabbitMQ.Client<br/>消息队列]
        NetEZ[NetEZ框架<br/>网络库]
    end

    subgraph "代码生成工具"
        CodeMakerCore[CodeMakerCore<br/>.NET Core版本]
        CodeMaker[CodeMaker<br/>.NET Framework版本]
    end

    CommunicatorFactory --> Communicator
    ObjectProxyBase --> Communicator
    ObjectProxyBase --> Transaction
    Communicator --> TcpClientPool
    Communicator --> JMPParser

    ClusterInvokerBase --> ClusterClient
    ClusterInvokerBase --> GridUri
    ClusterClient --> ClusterNodeInfo

    RPCServerHost --> RPCServerBase
    RPCServerBase --> ServantBase
    RPCServerBase --> ClusterClient

    Publisher --> RabbitMQClient
    Subscriber --> RabbitMQClient
    ExEventManager --> Publisher
    ExEventManager --> Subscriber

    ClusterClient -.-> ZooKeeper
    TcpClientPool -.-> NetEZ
    JMPParser -.-> NetEZ
    RabbitMQClient -.-> RabbitMQ

    CodeMakerCore -.生成.-> ObjectProxyBase
    CodeMaker -.生成.-> ObjectProxyBase

    style ZooKeeper fill:#f9f,stroke:#333,stroke-width:2px
    style RabbitMQ fill:#f9f,stroke:#333,stroke-width:2px
    style NetEZ fill:#f9f,stroke:#333,stroke-width:2px
```

**依赖说明：**
- **客户端层**：负责发起RPC调用和管理连接
- **服务端层**：负责接收请求和执行业务逻辑
- **集群层**：负责服务发现和负载均衡
- **通信层**：负责底层网络通信和消息解析
- **消息系统**：独立的发布/订阅事件系统
- **外部依赖**：ZooKeeper（集群）、RabbitMQ（消息）、NetEZ（网络）

---

## 7. 流程图 - 集群模式选择

展示根据 GridUri 和集群模式选择目标节点的决策逻辑。

```mermaid
flowchart TD
    Start([开始RPC调用]) --> ParseUri[解析GridUri<br/>rpc://cluster.project.root]
    ParseUri --> GetMode{获取集群模式}

    GetMode -->|None| DirectConnect[直连单个服务器<br/>使用配置的Host:Port]
    GetMode -->|MasterSlave| CheckRole{需要主节点?}
    GetMode -->|Cluster| CheckHash{是否有哈希键?}
    GetMode -->|ClusterWithHash| UseConsistentHash[使用一致性哈希<br/>GetClusterNodeByHash]

    CheckRole -->|是| GetMaster[GetMasterClusterNode<br/>选择主节点]
    CheckRole -->|否| GetSlave[GetSlaveClusterNode<br/>随机选择从节点]

    CheckHash -->|有| UseMod[使用取模哈希<br/>GetClusterNodeByMod]
    CheckHash -->|无| UseRandom[随机选择<br/>GetClusterNodeByRandom]

    DirectConnect --> GetComm[获取Communicator]
    GetMaster --> GetComm
    GetSlave --> GetComm
    UseMod --> GetComm
    UseRandom --> GetComm
    UseConsistentHash --> GetComm

    GetComm --> SendRequest[发送RPC请求]
    SendRequest --> End([结束])

    style GetMode fill:#ffe6cc
    style CheckRole fill:#ffe6cc
    style CheckHash fill:#ffe6cc
    style DirectConnect fill:#d5e8d4
    style GetMaster fill:#d5e8d4
    style GetSlave fill:#d5e8d4
    style UseMod fill:#d5e8d4
    style UseRandom fill:#d5e8d4
    style UseConsistentHash fill:#d5e8d4
```

**集群模式说明：**
- **None**：不使用集群，直连单个服务器
- **MasterSlave**：主从模式，读写分离，主节点处理写操作
- **Cluster**：多实例集群，使用取模或随机路由
- **ClusterWithHash**：一致性哈希，适合缓存等有状态服务

---

## 8. 流程图 - 重试逻辑

展示 ObjectProxyBase 中的重试机制和指数退避策略。

```mermaid
flowchart TD
    Start([开始发送请求]) --> Init[初始化重试计数<br/>retryCount = 0<br/>maxRetry = 2]
    Init --> Send[SendMessage]

    Send --> CheckSend{发送成功?}
    CheckSend -->|是| WaitAck[等待响应<br/>超时时间: 9秒]
    CheckSend -->|否| CheckRetry1

    WaitAck --> CheckAck{收到响应?}
    CheckAck -->|是| Success([返回成功结果])
    CheckAck -->|否 超时| CheckRetry2{retryCount < maxRetry?}

    CheckRetry1{retryCount < maxRetry?}
    CheckRetry1 -->|是| IncRetry1[retryCount++]
    CheckRetry1 -->|否| Fail([抛出异常<br/>发送失败])

    CheckRetry2 -->|是| IncRetry2[retryCount++]
    CheckRetry2 -->|否| Timeout([抛出异常<br/>响应超时])

    IncRetry1 --> Backoff1[指数退避延迟<br/>delay = 100ms * 2^retryCount]
    IncRetry2 --> Backoff2[指数退避延迟<br/>delay = 100ms * 2^retryCount]

    Backoff1 --> Sleep1[Thread.Sleep]
    Backoff2 --> Sleep2[Thread.Sleep]

    Sleep1 --> Send
    Sleep2 --> Send

    style Success fill:#d5e8d4
    style Fail fill:#f8cecc
    style Timeout fill:#f8cecc
    style CheckSend fill:#ffe6cc
    style CheckAck fill:#ffe6cc
    style CheckRetry1 fill:#ffe6cc
    style CheckRetry2 fill:#ffe6cc
```

**重试策略：**
- **最大重试次数**：默认2次（可配置）
- **退避策略**：指数退避，避免服务器过载
- **延迟计算**：100ms × 2^retryCount
  - 第1次重试：100ms
  - 第2次重试：200ms
  - 第3次重试：400ms

---

## 9. 状态图 - Transaction状态机

展示 Transaction 对象在 RPC 调用过程中的状态转换。

```mermaid
stateDiagram-v2
    [*] --> RESET: 创建Transaction

    RESET --> SEND_PENDING: BeginRpcTransaction()<br/>开始发送

    SEND_PENDING --> OK: OnSendCompleted()<br/>发送成功
    SEND_PENDING --> SEND_FAIL: OnSendCompleted()<br/>发送失败
    SEND_PENDING --> RESET: 超时或异常<br/>UnRegisterTransaction()

    OK --> ACK_PENDING: 等待服务器响应<br/>ack_evt.WaitOne()

    ACK_PENDING --> COMPLETE: OnReceivedServerData()<br/>收到响应
    ACK_PENDING --> RESET: 响应超时(9秒)<br/>UnRegisterTransaction()

    SEND_FAIL --> RESET: 重试或放弃<br/>UnRegisterTransaction()

    COMPLETE --> [*]: 返回结果<br/>UnRegisterTransaction()

    note right of RESET
        初始状态
        Transaction对象创建
    end note

    note right of SEND_PENDING
        正在发送请求
        等待send_evt
    end note

    note right of OK
        请求已发送
        网络层确认
    end note

    note right of ACK_PENDING
        等待服务器处理
        等待ack_evt
    end note

    note right of COMPLETE
        收到响应
        可以返回结果
    end note

    note right of SEND_FAIL
        发送失败
        可能重试
    end note
```

**状态说明：**
- **RESET**：初始状态，Transaction 刚创建
- **SEND_PENDING**：正在发送请求到服务器
- **OK**：请求已成功发送，等待响应
- **SEND_FAIL**：发送失败，可能触发重试
- **ACK_PENDING**：等待服务器处理并响应
- **COMPLETE**：收到响应，调用完成

**事件机制：**
- `send_evt`：AutoResetEvent，等待发送完成
- `ack_evt`：AutoResetEvent，等待响应到达

---

## 10. 架构总览图

展示 ExRpc 框架的整体架构和各层次之间的关系。

```mermaid
graph TB
    subgraph "客户端应用层"
        ClientApp[业务应用程序]
    end

    subgraph "客户端代理层"
        ProxyGen[生成的代理类<br/>ObjectProxyXxx]
        ProxyBase[ObjectProxyBase]
        ClusterInvoker[ClusterInvokerBase]
    end

    subgraph "通信管理层"
        CommFactory[CommunicatorFactory<br/>工厂模式]
        Communicator1[Communicator]
        TransTable[Transaction Table<br/>事务管理]
        ConnPool[TcpClientPool<br/>连接池]
    end

    subgraph "集群发现层"
        ClusterClient1[ClusterClient]
        ZK[(ZooKeeper<br/>服务注册中心)]
    end

    subgraph "网络传输层"
        Network[TCP/IP Network]
    end

    subgraph "服务端主机层"
        ServerHost[RPCServerHost<br/>TcpServiceBase]
    end

    subgraph "服务端处理层"
        ServerBase[RPCServerBase<br/>路由和调度]
        ServantTable[Servant Table<br/>业务处理器映射]
    end

    subgraph "业务逻辑层"
        Servant1[ServantBase 1<br/>业务实现]
        Servant2[ServantBase 2<br/>业务实现]
        Servant3[ServantBase N<br/>业务实现]
    end

    subgraph "服务端应用层"
        ServerApp[业务应用程序]
    end

    subgraph "消息队列层"
        EventMgr[ExEventManager<br/>事件管理]
        Publisher1[Publisher<br/>发布者]
        Subscriber1[Subscriber<br/>订阅者]
        MQ[(RabbitMQ<br/>消息队列)]
    end

    ClientApp --> ProxyGen
    ProxyGen --> ProxyBase
    ProxyBase --> Communicator1
    ClusterInvoker --> ClusterClient1
    ClusterInvoker --> Communicator1

    CommFactory -.创建.-> Communicator1
    Communicator1 --> TransTable
    Communicator1 --> ConnPool

    ClusterClient1 <-.监听.-> ZK
    ServerBase <-.注册.-> ZK

    ConnPool --> Network
    Network --> ServerHost

    ServerHost --> ServerBase
    ServerBase --> ServantTable
    ServantTable --> Servant1
    ServantTable --> Servant2
    ServantTable --> Servant3

    Servant1 --> ServerApp
    Servant2 --> ServerApp
    Servant3 --> ServerApp

    ClientApp -.发布事件.-> Publisher1
    ServerApp -.订阅事件.-> Subscriber1
    EventMgr -.管理.-> Publisher1
    EventMgr -.管理.-> Subscriber1
    Publisher1 --> MQ
    MQ --> Subscriber1

    style ClientApp fill:#e1f5ff
    style ServerApp fill:#e1f5ff
    style ZK fill:#fff4e6
    style MQ fill:#fff4e6
    style Network fill:#f0f0f0
    style CommFactory fill:#ffe6cc
    style EventMgr fill:#ffe6cc
```

**架构层次说明：**

1. **客户端应用层**：业务代码，调用远程服务
2. **客户端代理层**：自动生成的代理类，封装 RPC 调用
3. **通信管理层**：管理连接、事务、超时和重试
4. **集群发现层**：与 ZooKeeper 交互，获取服务节点列表
5. **网络传输层**：TCP/IP 网络通信
6. **服务端主机层**：监听端口，接收请求
7. **服务端处理层**：路由请求到具体的业务处理器
8. **业务逻辑层**：实际执行业务逻辑的 Servant
9. **服务端应用层**：业务代码，实现具体功能
10. **消息队列层**：独立的事件发布/订阅系统

**数据流向：**
- **RPC调用**：客户端 → 代理 → 通信器 → 网络 → 服务器 → Servant → 响应返回
- **集群发现**：ZooKeeper ← ClusterClient → 更新节点列表
- **事件消息**：发布者 → RabbitMQ → 订阅者

---

## 总结

### 核心设计模式

1. **工厂模式**：CommunicatorFactory、RabbitMQClientFactory
2. **单例模式**：ExEventManager、CommunicatorFactory
3. **对象池模式**：TcpClientPool、RabbitMQClientPool
4. **观察者模式**：ClusterClient 监听器、事件处理器
5. **模板方法模式**：ObjectProxyBase、ServantBase
6. **策略模式**：集群模式选择（Random、Hash、MasterSlave）
7. **代理模式**：ObjectProxyBase（动态代理生成）

### 关键技术特性

1. **并发安全**：ConcurrentDictionary 用于所有共享缓存
2. **异步处理**：Transaction 使用 AutoResetEvent 进行同步
3. **连接复用**：TcpClientPool 连接池（默认50个连接）
4. **自动重试**：指数退避策略，最大重试2次
5. **服务发现**：ZooKeeper 监听集群拓扑变化
6. **负载均衡**：支持4种集群模式
7. **性能监控**：PerformanceRecorder 跟踪调用耗时
8. **事务管理**：自动超时清理（默认60秒）

### 适用场景

- **微服务架构**：分布式服务间通信
- **集群部署**：多实例负载均衡
- **高并发场景**：连接池和异步处理
- **事件驱动**：RabbitMQ 解耦服务
- **服务发现**：ZooKeeper 动态拓扑管理

---

## 附录：快速参考

### 关键类速查

| 类名 | 职责 | 位置 |
|------|------|------|
| `Communicator` | 管理通信和事务 | ExRpc.Common |
| `ObjectProxyBase` | 客户端代理基类 | ExRpc.Common |
| `RPCServerBase` | 服务器基类 | ExRpc.Common |
| `ServantBase` | 业务逻辑基类 | ExRpc.Common |
| `ClusterClient` | 集群管理客户端 | ExRpc.Common |
| `Transaction` | RPC 事务 | ExRpc.Common |
| `Publisher` | 事件发布者 | ExRpc.Messaging |
| `Subscriber` | 事件订阅者 | ExRpc.Messaging |

### 配置文件速查

| 文件 | 用途 |
|------|------|
| `communicator.xml` | Communicator 配置（超时、重试、连接池） |
| `zookeeper.xml` | ZooKeeper 连接配置 |
| `rabbitmq.xml` | RabbitMQ 连接和事件配置 |
| `server.xml` | 服务器主机配置（端口、Servant） |

### 超时参数速查

| 参数 | 默认值 | 说明 |
|------|--------|------|
| ConnectTimeout | 200ms | TCP 连接超时 |
| SendTimeout | 3000ms | 发送请求超时 |
| WaitingACKTimeout | 9000ms | 等待响应超时 |
| TransactionExpire | 60s | 事务过期时间 |
| RetryCount | 2 | 最大重试次数 |
| ConnectionPoolSize | 50 | 连接池大小 |
